{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # Use numpy to make vector and matrix computations easier\n",
    "np.random.seed(1)  # Fixing the seed for the random number generator, for reproducible results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron(object):\n",
    "    \"\"\"\n",
    "    A simple aritificial neuron, processing an input vector and returning a corresponding activation.\n",
    "    Args:\n",
    "        num_inputs (int): The input vector size / number of input values.\n",
    "        activation_function (callable): The activation function defining this neuron.\n",
    "    Attributes:\n",
    "        W (ndarray): The weight values for each input.\n",
    "        b (float): The bias value, added to the weighted sum.\n",
    "        activation_function (callable): The activation function computing the neuron's output.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_inputs, activation_function):\n",
    "        super().__init__\n",
    "\n",
    "        # Randomly initializing the weight vector and the bias value (e.g., using a \n",
    "        # simplistic uniform distribution element between -1 and 1):\n",
    "        self.W = np.random.uniform(size=num_inputs, low=-1., high=1.)\n",
    "        self.b = np.random.uniform(size=1, low=-1., high=1.)\n",
    "\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward the input signal through the neuron, returning its activation value.\n",
    "        Args:\n",
    "            x (ndarray): The input vector, of shape `(1, num_inputs)`\n",
    "        Returns:\n",
    "            activation (ndarray): The activation value, of shape `(1, layer_size)`.\n",
    "        \"\"\"\n",
    "        z = np.dot(x, self.W) + self.b\n",
    "        return self.activation_function(z)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class represents a simple artificial neuron, able to receive a vector of input values, to merge and process them before returning an activation value. \n",
    "\n",
    "First, we instantiate our neuron. Let us create a *perceptron* taking 2 input values and using the step function for computing its activation. its weights and biases are random;y set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron's random weights = [-0.16595599  0.44064899 -0.99977125], and random bias = [-0.39533485]\n"
     ]
    }
   ],
   "source": [
    "# Perceptron input size:\n",
    "input_size = 3\n",
    "\n",
    "# step function (returns 0 if y<= 0, and 1 if y > 0):\n",
    "step_function = lambda y: 0 if y <= 0 else 1\n",
    "\n",
    "# Instantiating the perceptron:\n",
    "perceptron = Neuron(num_inputs=input_size, activation_function=step_function)\n",
    "print(\"Perceptron's random weights = {}, and random bias = {}\".format(perceptron.W, perceptron.b))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We randomly generate a random input vector of 3 values (i.e., a column-vector of (shape = (1,3)), to be fed to our neuron):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input vector: [[0.34556073 0.39676747 0.53881673]]\n"
     ]
    }
   ],
   "source": [
    "x = np.random.rand(input_size).reshape(1, input_size)\n",
    "print(f\"Input vector: {x}\")\n",
    "#print(\"Input vector : {}\".format(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now feed our perceptron with this input and display the corresponding activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron's output value given `x` : 0\n"
     ]
    }
   ],
   "source": [
    "y = perceptron.forward(x)\n",
    "print(\"Perceptron's output value given `x` : {}\".format(y))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this Neuron class, we implemented the mathematical model for neurons proposed by early A.I. scientists."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layering Neurons Together\n",
    "We presented how neurons can be organized into *layers*. We introduced the following model, mathematically wrapping together the operations done by such a neural layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedLayer(object):\n",
    "     \"\"\"A simple fully-connected NN layer.\n",
    "    Args:\n",
    "        num_inputs (int): The input vector size / number of input values.\n",
    "        layer_size (int): The output vector size / number of neurons in the layer.\n",
    "        activation_function (callable): The activation function for this layer.\n",
    "    Attributes:\n",
    "        W (ndarray): The weight values for each input.\n",
    "        b (ndarray): The bias value, added to the weighted sum.\n",
    "        size (int): The layer size / number of neurons.\n",
    "        activation_function (callable): The activation function computing the neuron's output.\n",
    "        x (ndarray): The last provided input vector, stored for backpropagation.\n",
    "        y (ndarray): The corresponding output, also stored for backpropagation.\n",
    "        derivated_activation_function (callable): The corresponding derivated function for backpropagation.\n",
    "        dL_dW (ndarray): The derivative of the loss, with respect to the weights W.\n",
    "        dL_db (ndarray): The derivative of the loss, with respect to the bias b.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7501ec888eafb9394fa1b50f83fb13c21f7643cf3f25fbc67cfc13e4c6ea1591"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
